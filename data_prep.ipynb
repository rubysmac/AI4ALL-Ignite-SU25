{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e4a7e4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a4ed78bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('mental_health_dataset.csv',engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7ec9e77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "# Encode 'Gender' (categorical) as numeric\n",
    "le_gender = LabelEncoder()\n",
    "df['Gender'] = le_gender.fit_transform(df['Gender'])\n",
    "\n",
    "df.drop(['Mood_Description', 'Sentiment_Score','Student_ID', 'Stress_Level', 'Anxiety_Score', 'Depression_Score'], axis=1, inplace=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e089aae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nrclex import NRCLex\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import plotly.express as px\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c0800e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Age  Gender   GPA                                  Daily_Reflections  \\\n",
      "0     23       2  2.52  Onto foreign do environmental anyone every nea...   \n",
      "1     19       1  2.74  Party but others visit admit industry country ...   \n",
      "2     21       0  3.53  Religious sure wait do chance decade according...   \n",
      "3     18       1  2.04            A task effect entire coach join series.   \n",
      "4     19       2  2.87  Knowledge several camera wait week write quali...   \n",
      "..   ...     ...   ...                                                ...   \n",
      "495   20       2  3.34  Land floor page trade social away animal cut e...   \n",
      "496   18       0  3.22  Almost wide majority technology positive parti...   \n",
      "497   23       2  2.86  Property answer method call law dream maybe mo...   \n",
      "498   18       0  2.45       Care can now outside real rest that perform.   \n",
      "499   18       2  3.52  Get turn Congress list mouth city decision eas...   \n",
      "\n",
      "     Sleep_Hours  Steps_Per_Day  Mental_Health_Status  \\\n",
      "0            6.8           4166                     2   \n",
      "1            5.1           4949                     2   \n",
      "2            8.3           7632                     2   \n",
      "3            8.2           5548                     2   \n",
      "4            5.9           3698                     0   \n",
      "..           ...            ...                   ...   \n",
      "495          5.4           8725                     2   \n",
      "496          4.5           3692                     0   \n",
      "497          8.2           6000                     2   \n",
      "498          6.0           5013                     1   \n",
      "499          5.6           5440                     0   \n",
      "\n",
      "                             Daily_Reflections_Cleaned  fear     anger  \\\n",
      "0    onto foreign environmental anyone every nearly...   0.0  0.000000   \n",
      "1            party others visit admit industry country   0.0  0.000000   \n",
      "2    religious sure wait chance decade according wa...   0.0  0.000000   \n",
      "3                 task effect entire coach join series   0.0  0.000000   \n",
      "4    knowledge several camera wait week write quali...   0.0  0.076923   \n",
      "..                                                 ...   ...       ...   \n",
      "495   land floor page trade social away animal cut end   0.0  0.000000   \n",
      "496  almost wide majority technology positive parti...   0.0  0.000000   \n",
      "497  property answer method call law dream maybe no...   0.0  0.000000   \n",
      "498                     care outside real rest perform   0.0  0.000000   \n",
      "499  get turn congress list mouth city decision eas...   0.0  0.000000   \n",
      "\n",
      "     anticipation     trust  surprise  sadness       joy   disgust  negative  \\\n",
      "0        0.000000  0.000000  0.000000      0.0  0.000000  0.000000  1.000000   \n",
      "1        0.000000  0.000000  0.000000      0.0  0.000000  0.000000  0.000000   \n",
      "2        0.250000  0.000000  0.250000      0.0  0.000000  0.000000  0.250000   \n",
      "3        0.000000  0.500000  0.000000      0.0  0.000000  0.000000  0.000000   \n",
      "4        0.153846  0.153846  0.076923      0.0  0.153846  0.076923  0.153846   \n",
      "..            ...       ...       ...      ...       ...       ...       ...   \n",
      "495      0.000000  1.000000  0.000000      0.0  0.000000  0.000000  0.000000   \n",
      "496      0.000000  0.250000  0.125000      0.0  0.250000  0.000000  0.000000   \n",
      "497      0.000000  0.500000  0.000000      0.0  0.000000  0.000000  0.000000   \n",
      "498      0.000000  0.333333  0.000000      0.0  0.000000  0.000000  0.000000   \n",
      "499      0.285714  0.285714  0.142857      0.0  0.142857  0.000000  0.000000   \n",
      "\n",
      "     positive  \n",
      "0    0.000000  \n",
      "1    1.000000  \n",
      "2    0.250000  \n",
      "3    0.500000  \n",
      "4    0.153846  \n",
      "..        ...  \n",
      "495  0.000000  \n",
      "496  0.375000  \n",
      "497  0.500000  \n",
      "498  0.666667  \n",
      "499  0.142857  \n",
      "\n",
      "[500 rows x 18 columns]\n"
     ]
    }
   ],
   "source": [
    "# Convert 'Daily_Reflections' into numerical emotion vectors\n",
    "\n",
    "from collections import defaultdict\n",
    "\n",
    "def get_emotion_vector(text):\n",
    "    if pd.isna(text) or not str(text).strip():\n",
    "        # Handle empty or NaN reflections\n",
    "        return defaultdict(float)\n",
    "    \n",
    "    doc = NRCLex(text)\n",
    "    emotion_scores = doc.raw_emotion_scores\n",
    "    total = sum(emotion_scores.values()) or 1  # avoid division by zero\n",
    "    \n",
    "    # Normalize scores\n",
    "    normalized = {emotion: count / total for emotion, count in emotion_scores.items()}\n",
    "    \n",
    "    # Make sure all emotions are represented\n",
    "    all_emotions = ['fear', 'anger', 'anticipation', 'trust', 'surprise',\n",
    "                    'sadness', 'joy', 'disgust', 'negative', 'positive']\n",
    "    full_vector = {emotion: normalized.get(emotion, 0.0) for emotion in all_emotions}\n",
    "    \n",
    "    return full_vector\n",
    "\n",
    "# Apply the function to each row\n",
    "emotion_vectors = df['Daily_Reflections'].apply(get_emotion_vector)\n",
    "\n",
    "# Convert list of dicts to a DataFrame\n",
    "emotion_df = pd.DataFrame(emotion_vectors.tolist())\n",
    "\n",
    "# Concatenate with the original df\n",
    "df_with_emotions = pd.concat([df, emotion_df], axis=1)\n",
    "\n",
    "print(df_with_emotions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9586ca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = ['Age','Gender', 'GPA', 'Sleep_Hours', 'Steps_Per_Day', 'fear', 'anger', 'anticipation', 'trust', 'surprise','sadness', 'joy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "eb4ddfad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>GPA</th>\n",
       "      <th>Daily_Reflections</th>\n",
       "      <th>Sleep_Hours</th>\n",
       "      <th>Steps_Per_Day</th>\n",
       "      <th>Mental_Health_Status</th>\n",
       "      <th>fear</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>trust</th>\n",
       "      <th>surprise</th>\n",
       "      <th>sadness</th>\n",
       "      <th>joy</th>\n",
       "      <th>disgust</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>2.52</td>\n",
       "      <td>Onto foreign do environmental anyone every nea...</td>\n",
       "      <td>6.8</td>\n",
       "      <td>4166</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>2.74</td>\n",
       "      <td>Party but others visit admit industry country ...</td>\n",
       "      <td>5.1</td>\n",
       "      <td>4949</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "      <td>3.53</td>\n",
       "      <td>Religious sure wait do chance decade according...</td>\n",
       "      <td>8.3</td>\n",
       "      <td>7632</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>2.04</td>\n",
       "      <td>A task effect entire coach join series.</td>\n",
       "      <td>8.2</td>\n",
       "      <td>5548</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>19</td>\n",
       "      <td>2</td>\n",
       "      <td>2.87</td>\n",
       "      <td>Knowledge several camera wait week write quali...</td>\n",
       "      <td>5.9</td>\n",
       "      <td>3698</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>0.153846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>20</td>\n",
       "      <td>2</td>\n",
       "      <td>3.34</td>\n",
       "      <td>Land floor page trade social away animal cut e...</td>\n",
       "      <td>5.4</td>\n",
       "      <td>8725</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>3.22</td>\n",
       "      <td>Almost wide majority technology positive parti...</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3692</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.375000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>2.86</td>\n",
       "      <td>Property answer method call law dream maybe mo...</td>\n",
       "      <td>8.2</td>\n",
       "      <td>6000</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>18</td>\n",
       "      <td>0</td>\n",
       "      <td>2.45</td>\n",
       "      <td>Care can now outside real rest that perform.</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5013</td>\n",
       "      <td>1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>3.52</td>\n",
       "      <td>Get turn Congress list mouth city decision eas...</td>\n",
       "      <td>5.6</td>\n",
       "      <td>5440</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.142857</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Age  Gender   GPA                                  Daily_Reflections  \\\n",
       "0     23       2  2.52  Onto foreign do environmental anyone every nea...   \n",
       "1     19       1  2.74  Party but others visit admit industry country ...   \n",
       "2     21       0  3.53  Religious sure wait do chance decade according...   \n",
       "3     18       1  2.04            A task effect entire coach join series.   \n",
       "4     19       2  2.87  Knowledge several camera wait week write quali...   \n",
       "..   ...     ...   ...                                                ...   \n",
       "495   20       2  3.34  Land floor page trade social away animal cut e...   \n",
       "496   18       0  3.22  Almost wide majority technology positive parti...   \n",
       "497   23       2  2.86  Property answer method call law dream maybe mo...   \n",
       "498   18       0  2.45       Care can now outside real rest that perform.   \n",
       "499   18       2  3.52  Get turn Congress list mouth city decision eas...   \n",
       "\n",
       "     Sleep_Hours  Steps_Per_Day  Mental_Health_Status  fear     anger  \\\n",
       "0            6.8           4166                     2   0.0  0.000000   \n",
       "1            5.1           4949                     2   0.0  0.000000   \n",
       "2            8.3           7632                     2   0.0  0.000000   \n",
       "3            8.2           5548                     2   0.0  0.000000   \n",
       "4            5.9           3698                     0   0.0  0.076923   \n",
       "..           ...            ...                   ...   ...       ...   \n",
       "495          5.4           8725                     2   0.0  0.000000   \n",
       "496          4.5           3692                     0   0.0  0.000000   \n",
       "497          8.2           6000                     2   0.0  0.000000   \n",
       "498          6.0           5013                     1   0.0  0.000000   \n",
       "499          5.6           5440                     0   0.0  0.000000   \n",
       "\n",
       "     anticipation     trust  surprise  sadness       joy   disgust  negative  \\\n",
       "0        0.000000  0.000000  0.000000      0.0  0.000000  0.000000  1.000000   \n",
       "1        0.000000  0.000000  0.000000      0.0  0.000000  0.000000  0.000000   \n",
       "2        0.250000  0.000000  0.250000      0.0  0.000000  0.000000  0.250000   \n",
       "3        0.000000  0.500000  0.000000      0.0  0.000000  0.000000  0.000000   \n",
       "4        0.153846  0.153846  0.076923      0.0  0.153846  0.076923  0.153846   \n",
       "..            ...       ...       ...      ...       ...       ...       ...   \n",
       "495      0.000000  1.000000  0.000000      0.0  0.000000  0.000000  0.000000   \n",
       "496      0.000000  0.250000  0.125000      0.0  0.250000  0.000000  0.000000   \n",
       "497      0.000000  0.500000  0.000000      0.0  0.000000  0.000000  0.000000   \n",
       "498      0.000000  0.333333  0.000000      0.0  0.000000  0.000000  0.000000   \n",
       "499      0.285714  0.285714  0.142857      0.0  0.142857  0.000000  0.000000   \n",
       "\n",
       "     positive  \n",
       "0    0.000000  \n",
       "1    1.000000  \n",
       "2    0.250000  \n",
       "3    0.500000  \n",
       "4    0.153846  \n",
       "..        ...  \n",
       "495  0.000000  \n",
       "496  0.375000  \n",
       "497  0.500000  \n",
       "498  0.666667  \n",
       "499  0.142857  \n",
       "\n",
       "[500 rows x 17 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "ade1f74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize:\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_with_emotions[numerical_cols] = scaler.fit_transform(df_with_emotions[numerical_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b8673a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_with_emotions.to_csv(\"df_with_emotions.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b4b3276",
   "metadata": {},
   "source": [
    "### Optional: Alternative methods to convert textual data into numerical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "9fa7bf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nrclex import NRCLex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "92f2da01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     Daily_Reflections  \\\n",
      "0    Onto foreign do environmental anyone every nea...   \n",
      "1    Party but others visit admit industry country ...   \n",
      "2    Religious sure wait do chance decade according...   \n",
      "3              A task effect entire coach join series.   \n",
      "4    Knowledge several camera wait week write quali...   \n",
      "..                                                 ...   \n",
      "495  Land floor page trade social away animal cut e...   \n",
      "496  Almost wide majority technology positive parti...   \n",
      "497  Property answer method call law dream maybe mo...   \n",
      "498       Care can now outside real rest that perform.   \n",
      "499  Get turn Congress list mouth city decision eas...   \n",
      "\n",
      "                             Daily_Reflections_Cleaned  \n",
      "0    onto foreign environmental anyone every nearly...  \n",
      "1            party others visit admit industry country  \n",
      "2    religious sure wait chance decade according wa...  \n",
      "3                 task effect entire coach join series  \n",
      "4    knowledge several camera wait week write quali...  \n",
      "..                                                 ...  \n",
      "495   land floor page trade social away animal cut end  \n",
      "496  almost wide majority technology positive parti...  \n",
      "497  property answer method call law dream maybe no...  \n",
      "498                     care outside real rest perform  \n",
      "499  get turn congress list mouth city decision eas...  \n",
      "\n",
      "[500 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess(text):\n",
    "    if pd.isnull(text):  # handle missing values\n",
    "        return \"\"\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = [t for t in tokens if t.isalpha()]  # keep words only\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    lemmas = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "# Apply to the column\n",
    "df['Daily_Reflections_Cleaned'] = df['Daily_Reflections'].apply(preprocess)\n",
    "\n",
    "print(df[['Daily_Reflections', 'Daily_Reflections_Cleaned']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "127af28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ability  able  accept  according  account  across       act  action  \\\n",
      "0        0.0   0.0     0.0   0.000000      0.0     0.0  0.000000     0.0   \n",
      "1        0.0   0.0     0.0   0.000000      0.0     0.0  0.000000     0.0   \n",
      "2        0.0   0.0     0.0   0.288305      0.0     0.0  0.309231     0.0   \n",
      "3        0.0   0.0     0.0   0.000000      0.0     0.0  0.000000     0.0   \n",
      "4        0.0   0.0     0.0   0.000000      0.0     0.0  0.000000     0.0   \n",
      "..       ...   ...     ...        ...      ...     ...       ...     ...   \n",
      "495      0.0   0.0     0.0   0.000000      0.0     0.0  0.000000     0.0   \n",
      "496      0.0   0.0     0.0   0.000000      0.0     0.0  0.000000     0.0   \n",
      "497      0.0   0.0     0.0   0.000000      0.0     0.0  0.000000     0.0   \n",
      "498      0.0   0.0     0.0   0.000000      0.0     0.0  0.000000     0.0   \n",
      "499      0.0   0.0     0.0   0.000000      0.0     0.0  0.000000     0.0   \n",
      "\n",
      "     activity  actually  ...  would     write  writer  wrong  yard  yeah  \\\n",
      "0         0.0       0.0  ...    0.0  0.000000     0.0    0.0   0.0   0.0   \n",
      "1         0.0       0.0  ...    0.0  0.000000     0.0    0.0   0.0   0.0   \n",
      "2         0.0       0.0  ...    0.0  0.000000     0.0    0.0   0.0   0.0   \n",
      "3         0.0       0.0  ...    0.0  0.000000     0.0    0.0   0.0   0.0   \n",
      "4         0.0       0.0  ...    0.0  0.256283     0.0    0.0   0.0   0.0   \n",
      "..        ...       ...  ...    ...       ...     ...    ...   ...   ...   \n",
      "495       0.0       0.0  ...    0.0  0.000000     0.0    0.0   0.0   0.0   \n",
      "496       0.0       0.0  ...    0.0  0.000000     0.0    0.0   0.0   0.0   \n",
      "497       0.0       0.0  ...    0.0  0.000000     0.0    0.0   0.0   0.0   \n",
      "498       0.0       0.0  ...    0.0  0.000000     0.0    0.0   0.0   0.0   \n",
      "499       0.0       0.0  ...    0.0  0.000000     0.0    0.0   0.0   0.0   \n",
      "\n",
      "     year  yes  yet  young  \n",
      "0     0.0  0.0  0.0    0.0  \n",
      "1     0.0  0.0  0.0    0.0  \n",
      "2     0.0  0.0  0.0    0.0  \n",
      "3     0.0  0.0  0.0    0.0  \n",
      "4     0.0  0.0  0.0    0.0  \n",
      "..    ...  ...  ...    ...  \n",
      "495   0.0  0.0  0.0    0.0  \n",
      "496   0.0  0.0  0.0    0.0  \n",
      "497   0.0  0.0  0.0    0.0  \n",
      "498   0.0  0.0  0.0    0.0  \n",
      "499   0.0  0.0  0.0    0.0  \n",
      "\n",
      "[500 rows x 864 columns]\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(df['Daily_Reflections_Cleaned'])\n",
    "\n",
    "print(pd.DataFrame(X_tfidf.toarray(), columns=tfidf.get_feature_names_out()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2402179a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "648f32fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-seek-crawler",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
