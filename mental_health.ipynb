{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4a7e4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a4ed78bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('mental_health_dataset.csv',engine='python')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ec9e77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Gender'] = df['Gender'].astype('category')\n",
    "df.drop(['Mood_Description', 'Sentiment_Score'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e089aae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nrclex import NRCLex\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import plotly.express as px\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c0800e6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Student_ID  Age  Gender   GPA  Stress_Level  Anxiety_Score  \\\n",
      "0             1   23   Other  2.52             5             20   \n",
      "1             2   19    Male  2.74             5              3   \n",
      "2             3   21  Female  3.53             5             11   \n",
      "3             4   18    Male  2.04             4             15   \n",
      "4             5   19   Other  2.87             1              2   \n",
      "..          ...  ...     ...   ...           ...            ...   \n",
      "495         496   20   Other  3.34             4              0   \n",
      "496         497   18  Female  3.22             2              7   \n",
      "497         498   23   Other  2.86             4             17   \n",
      "498         499   18  Female  2.45             4             14   \n",
      "499         500   18   Other  3.52             1              0   \n",
      "\n",
      "     Depression_Score                                  Daily_Reflections  \\\n",
      "0                   6  Onto foreign do environmental anyone every nea...   \n",
      "1                   7  Party but others visit admit industry country ...   \n",
      "2                  24  Religious sure wait do chance decade according...   \n",
      "3                  14            A task effect entire coach join series.   \n",
      "4                   4  Knowledge several camera wait week write quali...   \n",
      "..                ...                                                ...   \n",
      "495                21  Land floor page trade social away animal cut e...   \n",
      "496                 3  Almost wide majority technology positive parti...   \n",
      "497                 1  Property answer method call law dream maybe mo...   \n",
      "498                 0       Care can now outside real rest that perform.   \n",
      "499                 5  Get turn Congress list mouth city decision eas...   \n",
      "\n",
      "     Sleep_Hours  Steps_Per_Day  ...  fear     anger  anticipation     trust  \\\n",
      "0            6.8           4166  ...   0.0  0.000000      0.000000  0.000000   \n",
      "1            5.1           4949  ...   0.0  0.000000      0.000000  0.000000   \n",
      "2            8.3           7632  ...   0.0  0.000000      0.250000  0.000000   \n",
      "3            8.2           5548  ...   0.0  0.000000      0.000000  0.500000   \n",
      "4            5.9           3698  ...   0.0  0.076923      0.153846  0.153846   \n",
      "..           ...            ...  ...   ...       ...           ...       ...   \n",
      "495          5.4           8725  ...   0.0  0.000000      0.000000  1.000000   \n",
      "496          4.5           3692  ...   0.0  0.000000      0.000000  0.250000   \n",
      "497          8.2           6000  ...   0.0  0.000000      0.000000  0.500000   \n",
      "498          6.0           5013  ...   0.0  0.000000      0.000000  0.333333   \n",
      "499          5.6           5440  ...   0.0  0.000000      0.285714  0.285714   \n",
      "\n",
      "     surprise  sadness       joy   disgust  negative  positive  \n",
      "0    0.000000      0.0  0.000000  0.000000  1.000000  0.000000  \n",
      "1    0.000000      0.0  0.000000  0.000000  0.000000  1.000000  \n",
      "2    0.250000      0.0  0.000000  0.000000  0.250000  0.250000  \n",
      "3    0.000000      0.0  0.000000  0.000000  0.000000  0.500000  \n",
      "4    0.076923      0.0  0.153846  0.076923  0.153846  0.153846  \n",
      "..        ...      ...       ...       ...       ...       ...  \n",
      "495  0.000000      0.0  0.000000  0.000000  0.000000  0.000000  \n",
      "496  0.125000      0.0  0.250000  0.000000  0.000000  0.375000  \n",
      "497  0.000000      0.0  0.000000  0.000000  0.000000  0.500000  \n",
      "498  0.000000      0.0  0.000000  0.000000  0.000000  0.666667  \n",
      "499  0.142857      0.0  0.142857  0.000000  0.000000  0.142857  \n",
      "\n",
      "[500 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def get_emotion_vector(text):\n",
    "    if pd.isna(text) or not str(text).strip():\n",
    "        # Handle empty or NaN reflections\n",
    "        return defaultdict(float)\n",
    "    \n",
    "    doc = NRCLex(text)\n",
    "    emotion_scores = doc.raw_emotion_scores\n",
    "    total = sum(emotion_scores.values()) or 1  # avoid division by zero\n",
    "    \n",
    "    # Normalize scores\n",
    "    normalized = {emotion: count / total for emotion, count in emotion_scores.items()}\n",
    "    \n",
    "    # Make sure all emotions are represented\n",
    "    all_emotions = ['fear', 'anger', 'anticipation', 'trust', 'surprise',\n",
    "                    'sadness', 'joy', 'disgust', 'negative', 'positive']\n",
    "    full_vector = {emotion: normalized.get(emotion, 0.0) for emotion in all_emotions}\n",
    "    \n",
    "    return full_vector\n",
    "\n",
    "# Apply the function to each row\n",
    "emotion_vectors = df['Daily_Reflections'].apply(get_emotion_vector)\n",
    "\n",
    "# Convert list of dicts to a DataFrame\n",
    "emotion_df = pd.DataFrame(emotion_vectors.tolist())\n",
    "\n",
    "# Concatenate with the original df\n",
    "df_with_emotions = pd.concat([df, emotion_df], axis=1)\n",
    "\n",
    "print(df_with_emotions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9586ca57",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_cols = ['Age','GPA','Stress_Level', 'Sleep_Hours', 'Anxiety_Score', 'Depression_Score', 'Steps_Per_Day', 'joy', 'sadness', 'fear', 'anger', 'trust', 'anticipation', 'surprise', 'positive', 'negative']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ade1f74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize:\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "df_with_emotions[numerical_cols] = scaler.fit_transform(df_with_emotions[numerical_cols])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "b8673a69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Student_ID</th>\n",
       "      <th>Age</th>\n",
       "      <th>Gender</th>\n",
       "      <th>GPA</th>\n",
       "      <th>Stress_Level</th>\n",
       "      <th>Anxiety_Score</th>\n",
       "      <th>Depression_Score</th>\n",
       "      <th>Daily_Reflections</th>\n",
       "      <th>Sleep_Hours</th>\n",
       "      <th>Steps_Per_Day</th>\n",
       "      <th>...</th>\n",
       "      <th>fear</th>\n",
       "      <th>anger</th>\n",
       "      <th>anticipation</th>\n",
       "      <th>trust</th>\n",
       "      <th>surprise</th>\n",
       "      <th>sadness</th>\n",
       "      <th>joy</th>\n",
       "      <th>disgust</th>\n",
       "      <th>negative</th>\n",
       "      <th>positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.159999</td>\n",
       "      <td>Other</td>\n",
       "      <td>-0.566791</td>\n",
       "      <td>1.354829</td>\n",
       "      <td>1.607159</td>\n",
       "      <td>-0.916966</td>\n",
       "      <td>Onto foreign do environmental anyone every nea...</td>\n",
       "      <td>0.483148</td>\n",
       "      <td>-0.963883</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.589006</td>\n",
       "      <td>-0.403906</td>\n",
       "      <td>-0.754115</td>\n",
       "      <td>-0.959785</td>\n",
       "      <td>-0.409189</td>\n",
       "      <td>-0.401565</td>\n",
       "      <td>-0.739343</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.918226</td>\n",
       "      <td>-1.297918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>-0.601579</td>\n",
       "      <td>Male</td>\n",
       "      <td>-0.138201</td>\n",
       "      <td>1.354829</td>\n",
       "      <td>-1.074594</td>\n",
       "      <td>-0.796598</td>\n",
       "      <td>Party but others visit admit industry country ...</td>\n",
       "      <td>-0.633426</td>\n",
       "      <td>-0.566872</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.589006</td>\n",
       "      <td>-0.403906</td>\n",
       "      <td>-0.754115</td>\n",
       "      <td>-0.959785</td>\n",
       "      <td>-0.409189</td>\n",
       "      <td>-0.401565</td>\n",
       "      <td>-0.739343</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.558704</td>\n",
       "      <td>2.905716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.279210</td>\n",
       "      <td>Female</td>\n",
       "      <td>1.400826</td>\n",
       "      <td>1.354829</td>\n",
       "      <td>0.187407</td>\n",
       "      <td>1.249664</td>\n",
       "      <td>Religious sure wait do chance decade according...</td>\n",
       "      <td>1.468360</td>\n",
       "      <td>0.793510</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.589006</td>\n",
       "      <td>-0.403906</td>\n",
       "      <td>0.977859</td>\n",
       "      <td>-0.959785</td>\n",
       "      <td>2.005550</td>\n",
       "      <td>-0.401565</td>\n",
       "      <td>-0.739343</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.060529</td>\n",
       "      <td>-0.247009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>-1.041973</td>\n",
       "      <td>Male</td>\n",
       "      <td>-1.501896</td>\n",
       "      <td>0.655743</td>\n",
       "      <td>0.818408</td>\n",
       "      <td>0.045981</td>\n",
       "      <td>A task effect entire coach join series.</td>\n",
       "      <td>1.402679</td>\n",
       "      <td>-0.263157</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.589006</td>\n",
       "      <td>-0.403906</td>\n",
       "      <td>-0.754115</td>\n",
       "      <td>1.588991</td>\n",
       "      <td>-0.409189</td>\n",
       "      <td>-0.401565</td>\n",
       "      <td>-0.739343</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.558704</td>\n",
       "      <td>0.803899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>-0.601579</td>\n",
       "      <td>Other</td>\n",
       "      <td>0.115057</td>\n",
       "      <td>-1.441515</td>\n",
       "      <td>-1.232344</td>\n",
       "      <td>-1.157703</td>\n",
       "      <td>Knowledge several camera wait week write quali...</td>\n",
       "      <td>-0.107979</td>\n",
       "      <td>-1.201176</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.589006</td>\n",
       "      <td>0.271686</td>\n",
       "      <td>0.311715</td>\n",
       "      <td>-0.175546</td>\n",
       "      <td>0.333807</td>\n",
       "      <td>-0.401565</td>\n",
       "      <td>0.895866</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.437747</td>\n",
       "      <td>-0.651205</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>496</td>\n",
       "      <td>-0.161184</td>\n",
       "      <td>Other</td>\n",
       "      <td>1.030681</td>\n",
       "      <td>0.655743</td>\n",
       "      <td>-1.547845</td>\n",
       "      <td>0.888559</td>\n",
       "      <td>Land floor page trade social away animal cut e...</td>\n",
       "      <td>-0.436383</td>\n",
       "      <td>1.347702</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.589006</td>\n",
       "      <td>-0.403906</td>\n",
       "      <td>-0.754115</td>\n",
       "      <td>4.137768</td>\n",
       "      <td>-0.409189</td>\n",
       "      <td>-0.401565</td>\n",
       "      <td>-0.739343</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.558704</td>\n",
       "      <td>-1.297918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>497</td>\n",
       "      <td>-1.041973</td>\n",
       "      <td>Female</td>\n",
       "      <td>0.796904</td>\n",
       "      <td>-0.742429</td>\n",
       "      <td>-0.443594</td>\n",
       "      <td>-1.278071</td>\n",
       "      <td>Almost wide majority technology positive parti...</td>\n",
       "      <td>-1.027510</td>\n",
       "      <td>-1.204219</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.589006</td>\n",
       "      <td>-0.403906</td>\n",
       "      <td>-0.754115</td>\n",
       "      <td>0.314603</td>\n",
       "      <td>0.798180</td>\n",
       "      <td>-0.401565</td>\n",
       "      <td>1.917872</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.558704</td>\n",
       "      <td>0.278445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>498</td>\n",
       "      <td>1.159999</td>\n",
       "      <td>Other</td>\n",
       "      <td>0.095576</td>\n",
       "      <td>0.655743</td>\n",
       "      <td>1.133908</td>\n",
       "      <td>-1.518808</td>\n",
       "      <td>Property answer method call law dream maybe mo...</td>\n",
       "      <td>1.402679</td>\n",
       "      <td>-0.033976</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.589006</td>\n",
       "      <td>-0.403906</td>\n",
       "      <td>-0.754115</td>\n",
       "      <td>1.588991</td>\n",
       "      <td>-0.409189</td>\n",
       "      <td>-0.401565</td>\n",
       "      <td>-0.739343</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.558704</td>\n",
       "      <td>0.803899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>499</td>\n",
       "      <td>-1.041973</td>\n",
       "      <td>Female</td>\n",
       "      <td>-0.703160</td>\n",
       "      <td>0.655743</td>\n",
       "      <td>0.660658</td>\n",
       "      <td>-1.639176</td>\n",
       "      <td>Care can now outside real rest that perform.</td>\n",
       "      <td>-0.042298</td>\n",
       "      <td>-0.534422</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.589006</td>\n",
       "      <td>-0.403906</td>\n",
       "      <td>-0.754115</td>\n",
       "      <td>0.739399</td>\n",
       "      <td>-0.409189</td>\n",
       "      <td>-0.401565</td>\n",
       "      <td>-0.739343</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.558704</td>\n",
       "      <td>1.504504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>500</td>\n",
       "      <td>-1.041973</td>\n",
       "      <td>Other</td>\n",
       "      <td>1.381345</td>\n",
       "      <td>-1.441515</td>\n",
       "      <td>-1.547845</td>\n",
       "      <td>-1.037334</td>\n",
       "      <td>Get turn Congress list mouth city decision eas...</td>\n",
       "      <td>-0.305022</td>\n",
       "      <td>-0.317917</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.589006</td>\n",
       "      <td>-0.403906</td>\n",
       "      <td>1.225284</td>\n",
       "      <td>0.496659</td>\n",
       "      <td>0.970662</td>\n",
       "      <td>-0.401565</td>\n",
       "      <td>0.779065</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.558704</td>\n",
       "      <td>-0.697399</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Student_ID       Age  Gender       GPA  Stress_Level  Anxiety_Score  \\\n",
       "0             1  1.159999   Other -0.566791      1.354829       1.607159   \n",
       "1             2 -0.601579    Male -0.138201      1.354829      -1.074594   \n",
       "2             3  0.279210  Female  1.400826      1.354829       0.187407   \n",
       "3             4 -1.041973    Male -1.501896      0.655743       0.818408   \n",
       "4             5 -0.601579   Other  0.115057     -1.441515      -1.232344   \n",
       "..          ...       ...     ...       ...           ...            ...   \n",
       "495         496 -0.161184   Other  1.030681      0.655743      -1.547845   \n",
       "496         497 -1.041973  Female  0.796904     -0.742429      -0.443594   \n",
       "497         498  1.159999   Other  0.095576      0.655743       1.133908   \n",
       "498         499 -1.041973  Female -0.703160      0.655743       0.660658   \n",
       "499         500 -1.041973   Other  1.381345     -1.441515      -1.547845   \n",
       "\n",
       "     Depression_Score                                  Daily_Reflections  \\\n",
       "0           -0.916966  Onto foreign do environmental anyone every nea...   \n",
       "1           -0.796598  Party but others visit admit industry country ...   \n",
       "2            1.249664  Religious sure wait do chance decade according...   \n",
       "3            0.045981            A task effect entire coach join series.   \n",
       "4           -1.157703  Knowledge several camera wait week write quali...   \n",
       "..                ...                                                ...   \n",
       "495          0.888559  Land floor page trade social away animal cut e...   \n",
       "496         -1.278071  Almost wide majority technology positive parti...   \n",
       "497         -1.518808  Property answer method call law dream maybe mo...   \n",
       "498         -1.639176       Care can now outside real rest that perform.   \n",
       "499         -1.037334  Get turn Congress list mouth city decision eas...   \n",
       "\n",
       "     Sleep_Hours  Steps_Per_Day  ...      fear     anger  anticipation  \\\n",
       "0       0.483148      -0.963883  ... -0.589006 -0.403906     -0.754115   \n",
       "1      -0.633426      -0.566872  ... -0.589006 -0.403906     -0.754115   \n",
       "2       1.468360       0.793510  ... -0.589006 -0.403906      0.977859   \n",
       "3       1.402679      -0.263157  ... -0.589006 -0.403906     -0.754115   \n",
       "4      -0.107979      -1.201176  ... -0.589006  0.271686      0.311715   \n",
       "..           ...            ...  ...       ...       ...           ...   \n",
       "495    -0.436383       1.347702  ... -0.589006 -0.403906     -0.754115   \n",
       "496    -1.027510      -1.204219  ... -0.589006 -0.403906     -0.754115   \n",
       "497     1.402679      -0.033976  ... -0.589006 -0.403906     -0.754115   \n",
       "498    -0.042298      -0.534422  ... -0.589006 -0.403906     -0.754115   \n",
       "499    -0.305022      -0.317917  ... -0.589006 -0.403906      1.225284   \n",
       "\n",
       "        trust  surprise   sadness       joy   disgust  negative  positive  \n",
       "0   -0.959785 -0.409189 -0.401565 -0.739343  0.000000  5.918226 -1.297918  \n",
       "1   -0.959785 -0.409189 -0.401565 -0.739343  0.000000 -0.558704  2.905716  \n",
       "2   -0.959785  2.005550 -0.401565 -0.739343  0.000000  1.060529 -0.247009  \n",
       "3    1.588991 -0.409189 -0.401565 -0.739343  0.000000 -0.558704  0.803899  \n",
       "4   -0.175546  0.333807 -0.401565  0.895866  0.076923  0.437747 -0.651205  \n",
       "..        ...       ...       ...       ...       ...       ...       ...  \n",
       "495  4.137768 -0.409189 -0.401565 -0.739343  0.000000 -0.558704 -1.297918  \n",
       "496  0.314603  0.798180 -0.401565  1.917872  0.000000 -0.558704  0.278445  \n",
       "497  1.588991 -0.409189 -0.401565 -0.739343  0.000000 -0.558704  0.803899  \n",
       "498  0.739399 -0.409189 -0.401565 -0.739343  0.000000 -0.558704  1.504504  \n",
       "499  0.496659  0.970662 -0.401565  0.779065  0.000000 -0.558704 -0.697399  \n",
       "\n",
       "[500 rows x 21 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_with_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9fa7bf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "from nrclex import NRCLex\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92f2da01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     Daily_Reflections  \\\n",
      "0    Onto foreign do environmental anyone every nea...   \n",
      "1    Party but others visit admit industry country ...   \n",
      "2    Religious sure wait do chance decade according...   \n",
      "3              A task effect entire coach join series.   \n",
      "4    Knowledge several camera wait week write quali...   \n",
      "..                                                 ...   \n",
      "495  Land floor page trade social away animal cut e...   \n",
      "496  Almost wide majority technology positive parti...   \n",
      "497  Property answer method call law dream maybe mo...   \n",
      "498       Care can now outside real rest that perform.   \n",
      "499  Get turn Congress list mouth city decision eas...   \n",
      "\n",
      "                             Daily_Reflections_Cleaned  \n",
      "0    onto foreign environmental anyone every nearly...  \n",
      "1            party others visit admit industry country  \n",
      "2    religious sure wait chance decade according wa...  \n",
      "3                 task effect entire coach join series  \n",
      "4    knowledge several camera wait week write quali...  \n",
      "..                                                 ...  \n",
      "495   land floor page trade social away animal cut end  \n",
      "496  almost wide majority technology positive parti...  \n",
      "497  property answer method call law dream maybe no...  \n",
      "498                     care outside real rest perform  \n",
      "499  get turn congress list mouth city decision eas...  \n",
      "\n",
      "[500 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "def preprocess(text):\n",
    "    if pd.isnull(text):  # handle missing values\n",
    "        return \"\"\n",
    "    tokens = nltk.word_tokenize(text.lower())\n",
    "    tokens = [t for t in tokens if t.isalpha()]  # keep words only\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    lemmas = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "# Apply to the column\n",
    "df['Daily_Reflections_Cleaned'] = df['Daily_Reflections'].apply(preprocess)\n",
    "\n",
    "print(df[['Daily_Reflections', 'Daily_Reflections_Cleaned']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "127af28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     ability  able  about  above  accept  according  account  across  \\\n",
      "0        0.0   0.0    0.0    0.0     0.0   0.000000      0.0     0.0   \n",
      "1        0.0   0.0    0.0    0.0     0.0   0.000000      0.0     0.0   \n",
      "2        0.0   0.0    0.0    0.0     0.0   0.276562      0.0     0.0   \n",
      "3        0.0   0.0    0.0    0.0     0.0   0.000000      0.0     0.0   \n",
      "4        0.0   0.0    0.0    0.0     0.0   0.000000      0.0     0.0   \n",
      "..       ...   ...    ...    ...     ...        ...      ...     ...   \n",
      "495      0.0   0.0    0.0    0.0     0.0   0.000000      0.0     0.0   \n",
      "496      0.0   0.0    0.0    0.0     0.0   0.000000      0.0     0.0   \n",
      "497      0.0   0.0    0.0    0.0     0.0   0.000000      0.0     0.0   \n",
      "498      0.0   0.0    0.0    0.0     0.0   0.000000      0.0     0.0   \n",
      "499      0.0   0.0    0.0    0.0     0.0   0.000000      0.0     0.0   \n",
      "\n",
      "          act  action  ...  wrong  yard  yeah  year  yes  yet  you  young  \\\n",
      "0    0.000000     0.0  ...    0.0   0.0   0.0   0.0  0.0  0.0  0.0    0.0   \n",
      "1    0.000000     0.0  ...    0.0   0.0   0.0   0.0  0.0  0.0  0.0    0.0   \n",
      "2    0.296636     0.0  ...    0.0   0.0   0.0   0.0  0.0  0.0  0.0    0.0   \n",
      "3    0.000000     0.0  ...    0.0   0.0   0.0   0.0  0.0  0.0  0.0    0.0   \n",
      "4    0.000000     0.0  ...    0.0   0.0   0.0   0.0  0.0  0.0  0.0    0.0   \n",
      "..        ...     ...  ...    ...   ...   ...   ...  ...  ...  ...    ...   \n",
      "495  0.000000     0.0  ...    0.0   0.0   0.0   0.0  0.0  0.0  0.0    0.0   \n",
      "496  0.000000     0.0  ...    0.0   0.0   0.0   0.0  0.0  0.0  0.0    0.0   \n",
      "497  0.000000     0.0  ...    0.0   0.0   0.0   0.0  0.0  0.0  0.0    0.0   \n",
      "498  0.000000     0.0  ...    0.0   0.0   0.0   0.0  0.0  0.0  0.0    0.0   \n",
      "499  0.000000     0.0  ...    0.0   0.0   0.0   0.0  0.0  0.0  0.0    0.0   \n",
      "\n",
      "     your  yourself  \n",
      "0     0.0       0.0  \n",
      "1     0.0       0.0  \n",
      "2     0.0       0.0  \n",
      "3     0.0       0.0  \n",
      "4     0.0       0.0  \n",
      "..    ...       ...  \n",
      "495   0.0       0.0  \n",
      "496   0.0       0.0  \n",
      "497   0.0       0.0  \n",
      "498   0.0       0.0  \n",
      "499   0.0       0.0  \n",
      "\n",
      "[500 rows x 965 columns]\n"
     ]
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer()\n",
    "X_tfidf = tfidf.fit_transform(processed_texts)\n",
    "\n",
    "print(pd.DataFrame(X_tfidf.toarray(), columns=tfidf.get_feature_names_out()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2402179a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
